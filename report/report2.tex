\documentclass{DIKU-report-variant}

\usepackage[titelside, nat, en, farve]{ku-forside}
\opgave{Bacherlor Thesis}
\forfatter{Karl D. Asmussen}
\titel{Multidimensional Arrays}
\undertitel{Formally Speficied by Shape Description and Sliced with Affine Functions}
\vejleder{Supervisor: Jyrki Katajainen}
\dato{Delivered: June 2nd 2017}

\titlehead{Multidimensional Arrays: Formally Specified and Affinely Sliced} 
\authorhead{Karl D. Asmussen}

\usepackage[backend=biber]{biblatex}
%\usepackage{makeidx}
\addbibresource{refs.bib}
%\makeindex

\usepackage{fontspec}
\setmainfont{Times New Roman}
\setsansfont{Arial}
\setmonofont[Scale=0.8]{Fira Mono}
\usepackage[english]{babel}

\usepackage{color}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{listings}
\usepackage[linktocpage=true]{hyperref}
\usepackage{graphicx}

\lstdefinelanguage{Rust}{
  morekeywords={abstract, alignof, as, become, box, break, const, continue, crate, do, else, enum, extern, false, final, fn, for, if, impl, in, let, loop, macro, match, mod, move, mut, offsetof, override, priv, proc, pub, pure, ref, return, Self, self, sizeof, static, struct, super, trait, true, type, typeof, unsafe, unsized, use, virtual, where, while, yield},
  sensitive=true,
  morecomment=[l]{//},
  morecomment=[n]{/*}{*/},
  morestring=[b]",
  emph={Copy, Send, Sized, Sync, Drop, Fn, FnMut, FnOnce, Box, ToOwned, Clone, PartialEq, PartialOrd, Eq, Ord, AsRef, AsMut, Into, From, Default, Iterator, Extend, IntoIterator, DoubleEndedIterator, ExactSizeIterator, Option, Some, None, Result, Ok, Err, SliceConcatExt, String, str, ToString, Vec},
  %otherkeywords={::, ->, #, !, -, +, *, /, \%, \&, \&\&, ^, |, ||, <<, >>, =, ==, !=, <=, >=, <, >},
}
\lstset{ 
  backgroundcolor=\color{white},  
  basicstyle=\ttfamily,      
  breakatwhitespace=false,       
  breaklines=true,            
  captionpos=b,              
  commentstyle=\color{black!50}, 
  keepspaces=true,
  keywordstyle=\bfseries,
  language=Rust,
  numbers=none,                    
  numbersep=5pt,                  
  numberstyle=\tiny\ttfamily, 
  rulecolor=\color{black}, 
  showspaces=false,       
  showstringspaces=false,
  showtabs=false,       
  stepnumber=1,
  stringstyle=\color{black!70}, 
  tabsize=2,
  title=\lstname,
  belowskip=-1.5em,
}

\usepackage{tikz}
\usepackage{tikz-cd}
\usetikzlibrary{shapes,arrows,cd}

\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{wasysym}
\usepackage{bm}

\newcommand\mrm[1]{\mathrm{#1}}
\newcommand\brm[1]{\bm{\mrm{#1}}}
\newcommand\Nat{\mathbb{N}}
\newcommand\Real{\mathbb{R}}
\newcommand\Com{\mathbb{C}}
\newcommand\Arr[1]{{\brm{Arr}_{\brm{#1}}}}
\newcommand\SFR{\sharp\flat\varrho}
\newcommand\SSFR{\sigma\sharp\flat\varrho}
\newcommand\DSSFR{\div\sigma\sharp\flat\varrho}
\newcommand\ADSSFR{\approx\div\sigma\sharp\flat\varrho}
\newcommand\CADSSFR{c\cdot\approx\div\sigma\sharp\flat\varrho}
\newcommand\XCADSSFR{\times c\cdot\approx\div\sigma\sharp\flat\varrho}
\newcommand\impl{\mathrel{\Rightarrow}}
\newcommand\tlaf{\mathop{\rotatebox[origin=c]{180}{$\flat$}}}
\newcommand\reduce{\operatorname*{\brm{reduce}\,}}
\newcommand\diag{\operatorname*{\brm{diag}\,}}
\newcommand\tile{\operatorname*{\brm{tile}\,}}
\newcommand\oprodby[1]{\mathop{\operatorname*{\,\brm{by}}_{#1}}}

% TODO EDITING:
%   Scalar/unit array distinction
%   Full stops outside delimiting punctuation
%   Footnotes before full stops
%   Telegram paragraphs!!

\begin{document}
\maketitle

\begin{abstract}
  % overview
  We attempt to formalize a minimal definition of multidimensional arrays
  in a category theoretical setting and provide an example library implementation in Rust.
  % specific problem
  Many libraries exist to provide support for multidimensional data sets, but all
  of them are based on ad-hoc premises.
  % review of existing solutions
  Library implementations seen in NumPy, Matlab, Mathematica, and R are all
  perfectly serviceable, but all function on non-interoperable semantics.
  % outline of solution
  By formalizing a minimal set of operations on multidimensional arrays, we can
  describe the differing semantics of other libraries.
  % evaluation parameters
  In the end, we stress test knowledge and implementation, by applying it to
  a bachelor-level statistics problem and implementing a linear algebra algorithm.
\end{abstract}

\begin{keywords}
  Data structure, \(d\)-dimensional array, semantics, numerical computing, Rust, NumPy
\end{keywords}

\setcounter{tocdepth}{2}
\tableofcontents

\chapter{Formalization}

\section{Motivation}

Arrays of arrays are ubiquitous data structures, finding use
in computer vision, computer graphics, and numerical computation.
They model matrices in linear algebra, image data, and multidimensional data sets in statistical analysis.

They are found in various more-or-less user-friendly library implementations;
the least of which is the bare bones
\begin{center}
\ttfamily double my\_C\_array[\textit{N}][\textit{M}]
\end{center}
of C, or the
\begin{center}
\ttfamily real*8, dimension (0:\textit{N}-1, 0:\textit{M}-1) :: my\_F\_array
\end{center}
of Fortran\footnote{Fortran uses \(1\le i\le N\) indexing by default, whereas C uses \(0\le i<N\).
By using the `extent' syntax in Fortran the range of array index can be changed. I have elected
to base my arrays on zero-indexing for the reasons stated in \cite{EWD831}}.

More pleasant options are found in dedicated libraries and computer algebra systems: SciPy, Matlab,
R, Mathematica, and the like. These all provide excellent abstractions 
to eliminate the need for writing nested loops by hand, and provide
large collections of useful functions.
There are also the array programming languages, such as APL, J and K, which provide
much the same functionality, but in far fewer keystrokes.

The first and most obvious problems are that these libraries and languages are not interoperable
at the library level: apart from NumPy taking direct inspiration from Matlab, they
all have different ways to work with the data stored in arrays --- different semantics.
Even C and Fortran do not agree: the address of \texttt{my\_C\_array[0][0]} is `next
to' \texttt{my\_C\_array[0][1]}, while the same is true of \texttt{my\_F\_array (0, 0)}
and \texttt{my\_F\_array (1, 0)}.

This anarchy of convention is indicative of the way each of these libraries and
languages came about: ad hoc. Each has been concerned with practical applications
at every step of their design, perhaps largely disregarding any underlying theory.

In the absence of theory, many designs have arisen, seemingly with little common
ground. It is reasonable to think there is a unifying theory behind these many associated
models, and that is what we will explore in this chapter.

\section{A Working Definition of Arrays}

\begin{definition}
  \label{def:finset}
  A bounded subset of the naturals is
  the set of natural numbers less than a given number.
  We denote such sets \(\Nat_{<k}\), where \(k\) is the number
  all elements are less than. In set-builder notation it is the set \(\{ x \in \Nat \mid x < k \}\).
\end{definition}
\begin{observation}
  \label{ob:finset}
  Some observations on Definition~\ref{def:finset}:
  \begin{itemize}
    \item The set of natural numbers less than 1, \(\Nat_{<1}\) is the singleton set \(\{0\}\).
    \item The set of natural numbers less than 0, \(\Nat_{<0}\) is the empty set.
  \end{itemize}
\end{observation}

\begin{definition}
  \label{def:finseq}
  A finite sequence \(s\) drawn from a set \(S\) is a function mapping the natural
  numbers less than the length of the sequence, to elements of that set,
  in morphism notation written \(s : \Nat_{<k} \to S\),
\end{definition}

\begin{definition}
  \label{def:cuboid}
  An \(d\)-dimensional finite natural cuboid is the set of all natural-numbered points
  within an cuboid in \(d\)-dimensional space, which has one corner at the origin.

  In other words it is the Cartesian product of \(d\) sets of natural numbers
  less than various constants; in Cartesian product notation
  \(\Nat_{<k_0} \times \Nat_{<k_1} \times \cdots \times \Nat_{<k_{d-1}}\).

  The constituent finite sequences \(\Nat_{<k_i}\) are called its axes.

  An element of a cuboid is a coordinate, and the individual entries in
  the \(d\)-tuple that is the coordinate, are called components or indexes.
\end{definition}
\begin{observation}
  \label{ob:cuboid}
  Some observations on Definition~\ref{def:cuboid}:
  \begin{itemize}
    \item The set of natural numbers less than \(k\) is itself a 1-dimensional finite natural cuboid.
    \item The 0-dimensional natural cuboid is the singleton set \(\{()\}\).
    \item A natural cuboid wherein all the upper bounds for each axis is 1, that
      is to say \(\Nat_{<1} \times \Nat_{<1} \times \cdots \times \Nat_{<1}\), is
      also a singleton set \(\{(0, 0, \dots, 0)\}\).
    \item A natural cuboid where one axis has upper bound 0, that is to
      say, one of the axes is an empty set; is itself an empty set. This is very rarely relevant.
    \item The cardinality of a finite natural cuboid is equal to the product of
      the cardinalities of the constituent finite sequences
      \begin{align*}
      |\Nat_{<k_0} \times \cdots \times \Nat_{<k_{d-1}}| = \prod_{i=0}^{d-1} k_i
      \end{align*}
  \end{itemize}
\end{observation}

\begin{definition}
  \label{def:array}
  An rank-\(d\) array \(A\) over a set \(S\) generalizes a finite sequence and
  is a function from an \(d\)-dimensional cuboid
  \(\Nat_{<k_0} \times \Nat_{<k_1} \times \cdots \times \Nat_{<k_(d-1)}\) to elements of the set.

  In morphism notation, \(A : \Nat_{<k_0} \times \Nat_{<k_1} \times \cdots \times \Nat_{<k_{d-1}} \to S\).
\end{definition}
\begin{observation}
  \label{ob:array}
  Some observations on Definition~\ref{def:array}:
  \begin{itemize}
    \item All finite sequences are rank-1 arrays.
    \item An array where the domain is a singleton set as per Definition~\ref{def:cuboid} above,
      is effectively a scalar. We will distinguish scalars and singleton arrays for clarity.
  \end{itemize}
\end{observation}

\begin{remark}
  \label{rem:syntax}
  For the remainder of this report, we shall adopt a familiar syntax for arrays
  of ranks 1 and 2:
  \begin{align*}
    A = [ x_0\; x_1\; \cdots \; x_{d-1} ] \quad \text{and} \quad
    B = \begin{bmatrix}
      y_{0,0} & \cdots & y_{0,g-1} \\
      \vdots & \ddots & \vdots \\
      y_{c-1,0} & \dots  & y_{c-1,g-1} 
    \end{bmatrix}
  \end{align*}
  where
  \begin{align*}
    \sharp A = [d] \quad \text{and} \quad \sharp B = [ c\; g ] && .
  \end{align*}
  Indexing of arrays will be denoted with function-call notation, as in Fortran
  \begin{align*}
    A(n) = x_n \quad \text{and} \quad B(m,n) = y_{m,n} && .
  \end{align*}
\end{remark}

\begin{definition}
  \label{def:shape}
  Given an \(d\)-dimensional array \(A\) over a cuboid
  \(\Nat_{<k_0} \times \Nat_{<k_1} \times \cdots \times \Nat_{<k_{d-1}}\),
  the finite sequence (rank-1 array) of the upper limits of each dimension of the cuboid
  \(k_0, k_1, \dots, k_{d-1}\) is called the \emph{shape} of the array, denoted
  \(\sharp A\), pronounced `shape of A', in morphism notation \(\sharp A : \Nat_{<d} \to \Nat\). 
\end{definition}
\begin{observation}
  \label{ob:shape}
  Some observations on Definition~\ref{def:shape}:
  \begin{itemize}
    \item The shape of the shape is the rank, \(\sharp \sharp A = [d]\).
    \item The shape of the shape of the shape is always 1, meaning \(\sharp \sharp \sharp A = [1]\).
    \item The shape of a singleton array may be a finite sequence of 1's of any length (including 0).
  \end{itemize}
\end{observation}

\begin{example}
  Some examples of the shapes of arrays
  \begin{align*}
    \sharp [1\; 2\; 3] &= [3] \\
    \sharp [1\; 2\; 3\; 4\; 5\; 6] &= [6] \\
    \sharp \begin{bmatrix}
      1 & 2 & 3 \\ 4 & 5 & 6
    \end{bmatrix} &= [2\; 3] \\
    \sharp \begin{bmatrix}
      1 & 2 \\ 3 & 4 \\ 5 & 6
    \end{bmatrix} &= [3\; 2] \\
  \end{align*}
\end{example}

\begin{remark}
  \label{rem:quant-elision}
  When using operator notation \(\bigoplus_{i=m}^n f(i)\),
  we shall elide the bounds \({}_{i=m}^n\) when the range of quantification can
  be inferred from the domain of the function \(f\). If the function only
  takes one argument as well, we will even elide naming the quantified variable,
  emulating the `point-free' style of functional programming.

  For an rank-1 array \(A\) with shape \(k = \sharp A\), normally we would
  write the product over all elements in the array by specifying the lower
  and upper bounds of the product \(\prod_{i=0}^{k} A(i)\). Instead we elide
  that, and write \(\prod_i A(i)\) or even \(\prod A\).
\end{remark}

\begin{definition}
  \label{def:flat}
  The flat traversal of an array \(A\) is a finite sequence obtained by
  traversing the domain of \(A\) in lexicographical order.
  
  The standard lexicographical order of finite cuboid
  \(\Nat_{<k_0} \times \cdots \times \Nat_{<k_{d-1}}\) counts upwards by the
  last axis; that is to say in the order \((\dots,0), (\dots,1),\) \((\dots,2), \dots\);
  the last axis is thus the `most significant.'

  Of note is also the colexicographical order counts upwards by the first
  axis, making the last axis the most significant: \((0,\dots), (1,\dots), (2,\dots), \dots\).

  The flat traversal of the array is then the composition of the finite
  sequence of points in the cuboid given by a lexicographical order,
  with the array \(A\), yielding a finite sequence --- a rank-1 array.

  We denote the lexicographical or `flat' traversal by \(\flat A\).
\end{definition}
\begin{observation}
  \label{ob:flat}
  Some observations on Definition~\ref{def:flat}
  \begin{itemize}
    \item The traversal of a rank-1 array is itself, \(\sharp \sharp A = [1] \impl \flat A = A\).
    \item The shape of the flat traversal of an array is the product over the shape
       of said array \(\sharp \flat A = \left[\prod \sharp A\right]\).
  \end{itemize}
\end{observation}

\begin{observation}
  \label{ob:cardinality}
  The cardinality of a function, when given as a set of pairs, is equal to
  the cardinality of its domain. Interpreting an array \(A\) as a function, and
  taking its cardinality \(|A|\) is to take the cardinality of the cuboid it is
  defined over, which is the product of the cardinalities of constituent axes as
  per Definition \ref{def:cuboid}.

  Hence we have that \(|A| = \prod \sharp A\).
  We shall however prefer \(\sharp \flat A\), as it derives directly from
  the array concepts.
\end{observation}

\begin{example}
  \label{ex:flat}
  The flat traversal of the array
  \begin{align*}
    A =
    \begin{bmatrix}
      1 & 2 & 3 \\
      4 & 5 & 6 \\
      7 & 8 & 9
    \end{bmatrix} \quad \sharp A = [3\; 3]
  \end{align*}
  is \([1\; 2\; 3\; 4\; 5\; 6\; 7\; 8\; 9]\).
  The colex traversal is \([1\; 4\; 7\; 2\; 5\; 8\; 3\; 6\; 9]\).
\end{example}

\begin{observation}
  \label{ob:c-fortran-order}
  The lexicographical and colexicographical traversals of an array to some extent
  mimic reinterpreting an array as a sequence of contiguous cells in memory.

  By definition, our arrays are mathematical objects, and therefore have no
  implementation or embedding in computer memory, but even so, the lexicographical
  traversal corresponds to the C-layout and the colexicographical
  traversal is the Fortran-layout.

  The colexicographical traversal is rarely useful, save to illustrate
  this point.
\end{observation}

\begin{definition}
  \label{def:reshape-comp} 
  Two array shapes, \(\sharp A\) and \(\sharp B\) are reshape-compatible, iff the product
  of their entries are equal. In other words
  \(\prod_{i} (\sharp A)(i) = \prod_{i} (\sharp B)(i)\) (or even simpler,
  \(\prod \sharp A = \prod \sharp B\).)

  Reshape-compatibility is an equivalence relation.
\end{definition}

\begin{definition}
  \label{def:reshape-equiv}
  Two arrays are the reshapes of one another when they have the same traversal
  \(\flat A = \flat B\). This of course implies reshape-compatibility.
\end{definition}

\begin{definition}
  \label{def:reshape}
  Given an array \(A\) and a reshape-compatible shape \(\rho\), meaning \(\prod \sharp A = \prod\rho\),
  there is a unique array \(B\) which is also reshape-compatible \(\sharp B = \rho\) and
  has the same traversal as the first array \(\flat A = \flat B\).
  
  The array \(B\) is called the \(\rho\)-shaped reshape of \(A\), and we denote
  it \(B = \varrho(A, \rho)\).
\end{definition}

\begin{observation}
  \label{ob:reshape}
  Some observations on Definition~\ref{def:reshape}:
  \begin{itemize}
    \item The reshape operation is a partial \emph{function}. Given an array \(A\) and a compatible
      shape \(\rho\), there is only one possible array \(B\) that fulfills
      both \(\sharp B = \rho\) and \(\flat A = \flat B\).
    \item The reshape operation is a \emph{partial} function, in that it is defined exactly when the
      shape \(\rho\) is reshape compatible with \(\sharp A\) and undefined otherwise.
    \item Reshaping to one axis is the same as flattening, \(\varrho(A, \sharp \flat A) = \flat A\).
  \end{itemize}
\end{observation}

\begin{example}
  \label{ex:reshape}
  Reshape is a useful operation especially to create rank-\(d\) arrays
  out of easier-to-describe arrays, such as rank-1 arrays:
  \begin{align*}
    A &= [0\; 1\; 2\; 3\; 4\; 5] \\
    \varrho(A, [2\;3]) &= \begin{bmatrix}
      0 & 1 & 2 \\
      3 & 4 & 5 
    \end{bmatrix}
  \end{align*}
\end{example}

\section{Categorical Setting}

The problem of constructing a categorical setting for any theory
is that a bottom-up approach is often the last thing we want to do.

A bottom-up approach to formalizing something in category theory is to
start with a set-theoretical description of your objects
and then work with a subset of the functions between the underlying sets as
the arrows of your category.

This approach is used in e.g. group theory, ring theory, etc.

Unfortunately, our description of arrays is both not obviously workable from a
set-theoretical perspective, the set of arrays over a set is not characterized by
external propositions\footnote{Arrays over a set has is more in common with a slice category.}.

So, in the absence of a bottom-up approach, starting in set-theory and building category theory,
we do the opposite: start in category theory, and approach the desired properties of our arrays\footnote{This technique is often seen in group theory, where some groups are easily described as
free groups of some order, with additional equalities to obtain the desired structure.}.

\begin{definition}
  \label{def:category-of-shapes}
  We define the category \(\Arr\SFR\) as follows:

  The objects are finite sequences (rank-1 arrays)
  of natural numbers, \(o : \Nat_{<k} \to \Nat\).

  The arrows are as follows:
  \begin{itemize}
    \item For every object \(o\)
      there is an arrow \(\flat\) from it, to the single-element sequence
      that is the product of its elements \(\flat : o \to \big[ \prod o \big]\)
    \item For every sequence \(o\) of \(k\) elements,
      there is an arrow \(\sharp\) from it, to the single-element sequence
      that contains as its sole element, the number of elements in \(o\), in
      other words, \(\sharp : o \to [k]\).
    \item For every pair of sequences \(o, \hat o\), which have the same product
      \(\prod o = \prod \hat o\), an isomorphic arrow \(\varrho\) connects them
      \( \varrho : o \to \hat o\), \(\varrho^{-1} : \hat o \to o\).
  \end{itemize}

  There is an additional commutation which is a reflection of the
  property that \(\flat\) and \(\varrho\) are equivalent.
  The following diagram commutes
    \footnote{Recall that a commuting diagram is a statement of equivalences. Every composition
    of arrows in a commuting diagram with the same start and end (domain and codomain) are equal.}:
  \begin{center}
  \begin{tikzcd}
    A \ar [yshift=0.7ex, r, "\varrho"]
      \ar [yshift=-0.7ex, r, swap, "\flat"]
    & B
  \end{tikzcd}
  \end{center}
  and also that every \(\varrho\) arrow is an isomorphism, so the following commutes:
  \begin{center}
  \begin{tikzcd}
    A \ar [yshift=0.7ex, r, "\varrho"]
    & B \ar [yshift=-0.7ex, l, "\varrho"]
  \end{tikzcd}.
  \end{center}

  Define the category \(\Arr\varrho\) as the proper subcategory of \(\Arr\SFR\) without the
  \(\sharp\) morphisms (the \(\flat\) morphisms is just another name for certain \(\varrho\) morphisms.)

  We call \(\Arr\varrho\) the category of reshaping, and \(\Arr\SFR\) the category of array
  shapes proper.
\end{definition}

We see that the category of reshaping \(\Arr\varrho\) has a peculiar structure,
in that we have several clusters of isomorphic objects. All of these clusters are characterized by the property
that all of the objects in a cluster, which are finite sequences, have the same product.

If we take the skeleton category\footnote{Quotient under the equivalence relation that
two objects are equivalent iff an isomorphism exists between them.} of \(\Arr\varrho\), we do indeed
the trivial category over \(\Nat\) --- each object is a natural number, and there are no morphisms
save the identity morphism.

\section{Categorical Intuitions}

If we consider the category of array re-shapes \(\Arr\varrho\) and disregard the troublesome
\(\sharp\)-arrows (so, essentially array shapes without shape, but let us call it \(\Arr0\setminus\sharp\))
we have a category of \emph{factorizations}, isomorphic under the number factorized.

Let us take a closer look at what that means: 

Recall from elementary arithmetic that a factorization is a product of
natural numbers, and that two products are equal iff the identities of
the multiplication monoid proves that they are, or equivalently,
if they factorize the same number. A factorization is always equal to
the number factorized under the identities of multiplication.

This interpretation, of array shapes as factorizations make sense. We have
already singled out the \(\flat\)-arrows, which link each factorization to
its `parent' number.

It also opens up the consideration, that if array flattening, \(\flat A\) is
a useful operation, and it is grounded in properties of natural number multiplication.

Associativity of multiplication is taken as a given in that we define our
products as sequences of natural numbers, rather than binary trees with natural-numbered
leaves.

Commutativity of multiplication states that the order of operands is irrelevant,
and we can take a spin on it in this setting in the following manner:

\begin{definition}
  \label{def:category-of-transposes}
  Let \(\Arr{\SSFR}\) be a proper supercategory to \(\Arr\SFR\).
  It has an extra schema of arrows, as follows:

  If two objects \(A, B\) have their finite sequences be
  a permutation of one another, with the permutation given by \(\sigma_z\),
  there is an ismophic arrow connecting them
  \begin{center}
    \begin{tikzcd}
      A \ar [r, yshift=0.7ex, "\sigma_z"] & B \ar [l, yshift=-0.7ex, "{\sigma_z}^{-1}"]
    \end{tikzcd}
  \end{center}
  
  In the case that the permutation is the identity permutation, then the
  resulting \(\sigma\)-arrow is the identity arrow.

  \begin{center}
    \begin{tikzcd}
      A \ar [r, yshift=0.7ex, "\brm{id}"] \ar [r, swap, yshift=-0.7ex, "\sigma_{id}"] & A
    \end{tikzcd}
  \end{center}

  If there are more than one way to permute one object into another, there is
  a distinct arrow for each way. In other words, there exist non-trivial self-permutations.
  Given \(A, B\) and distinct permutations \(\sigma_x, \sigma_y : A \to B\), there exists
  the permutations \(\sigma_a : A \to A\) and \(\sigma_b : B \to B\), at least one of which
  is nontrivial, such that the following diagram commutes:

  \begin{center}
    \begin{tikzcd}
      A \ar [r, "\sigma_x"] \ar [d, "\sigma_a"] & B \\
      A \ar [r, "\sigma_y"] & B \ar [u, "\sigma_b"]
    \end{tikzcd}
  \end{center}

  We shall call \(\Arr{\SSFR}\) the category of array shapes with transpositions.
\end{definition}

As has now been demonstrated: in describing multiplication as finite sequences because of association,
commutativity becomes permutation.

Recall now that we are not working merely with products, but with array
shapes. One has to ask what exchanging the order of the dimensions means in
an array setting, and the answer presents itself in elementary linear algebra.

A matrix \(\brm M\) is a rank-2 array, with the shape \(\sharp \brm M = [m\;n]\).
It's transpose has rows and columns exchanged, meaning \(\sharp( \brm M^\intercal) = [n\;m]\)
--- the shape is permuted.

Permutation of the shape of an array is a generalization of the transpose of matrices.
\footnote{The conjugate transpose is also a generalization of the transpose, but it
is conceptually orthogonal to this.}
This is \emph{very} suggestive that this is a constructive avenue of modeling.

\section{Division and Multiplication}

Division over the natural numbers is
a partial function. \(\frac a b\) is only well defined if \(b\mid a\).

We shall model a subset of this definition as follows:

\begin{definition}
  \label{def:category-of-reductions}
  Let \(\Arr\DSSFR\) be a proper supercategory of \(\Arr\SSFR\).
  It has an extra schema of arrows as follows:
  
  For an object \(A\), let \(A'\) be the product obtained by deleting
  one element from \(A\), meaning
  \begin{align*}
    A = a_0 \times\cdots\times a_{k-1} \times &\,a_k \times a_{k+1}\times\cdots\times a_{d-1} \\
    A' = a_0 \times\cdots\times a_{k-1} &\times a_{k+1}\times\cdots\times a_{d-2}
  \end{align*}
  then there is an arrow from \(A\) to \(A'\), denoting the elimination of one
  of the factors in \(A\) by way of division.

  \begin{center}
    \begin{tikzcd}
      A \ar [r, "\div_k"] & A'
    \end{tikzcd}
  \end{center}

  Notice that for a product with consecutive repeated
  factors \[a_0 \times \cdots \times a_k \times a_{k+1} \times \cdots \times a_{d-1},\; a_k = a_{k+1}\]
  there are two distinct division arrows for removing \(a_k\) and \(a_{k+1}\), even
  though both operations achieve the same final result.

  These arrows signify `immediately obvious' division.
  We shall call \(\Arr\DSSFR\) the category of array shapes (\(\sharp\flat\varrho\)) with
  transposes (\(\sigma\)) and reductions (\(\div\)).
\end{definition}

The deletion of an element of the shape in this manner can model many things.

As an example,
consider the rank-2 array \(A = \left[\begin{smallmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{smallmatrix}\right]\).
We have that \(\sharp A = [2\; 3]\). Selecting only the first row \([1\;2\;3]\) yields an array of shape \([3]\),
meaning we have eliminated the 2 --- it has been `divided out.'

This example of array slicing is evocative and seems correct, but ultimately does not fit our categorical
definition: there are two distinct operations with which to slice \(A\) into a shape of 3 --- the first row,
and the second row. The definition states that there is only one.

Consider instead an alternative: given a rank-1 array \(B = [2\;3\;4]\) the sum of the
elements of \(B\) is 9, and \(\sharp [9] = [1]\). Contrary to indexing/slicing the array, of which there are
three different outcomes, there is only one sum. This fits better.

Now, one must also consider that one can take the product of the elements of \(B\) (yielding 24) which
is a troublesome second way of reducing \(B\) to a scalar. However, the choice of an operation --- summation,
product, minimum, maximum --- is less consequential than the choice of index: the categories of
array shapes with `friends' do not concern themselves with the semantics of any operations on the elements,
not even their types.

For the time being, we shall simply group all summation-like operations under the heading `reduction,' and
it will be shown later why this is useful and elegant.

A less general operation that finds frequent use in number theory is the radical of an integer. The
radical of an integer \(n\) is precisely the product of all distinct prime factors of \(n\). A radical
is thus also a square free number: it has no divisors that are themselves squares.

If we interpret this in a manner similar to division above, and create an `immediately obvious' form
of finding the radical, we can take inspiration from the wording of `square free' and propose eliminating
a duplicate factor in a product like so:

\begin{definition}
  \label{def:category-of-diagonals}
  Let \(\Arr\ADSSFR\) be a proper supercategory of \(\Arr\DSSFR\).
  It has an extra schema of arrows as follows:

  For an object \(A = a_0 \times \cdots \times a_{k-1}\) which has repeat factors
  \(a_p = a_q\). Let \(A'\) be the product obtained by deleting
  \(a_p\) from \(A\). Let \(A''\) be the product obtained by deleting \(a_q\) from \(A\).
  There is an arrow from \(A\) to \(A'\), and one from \(A\) to \(A''\).
  There is also an appropriate permutation to make the diagram commute:
  \begin{center}
    \begin{tikzcd}
      A \ar[r, "p\approx q"] \ar[dr, swap, "q\approx p"] & A' \\
      & A'' \ar[u, swap, "\sigma"]
    \end{tikzcd}
  \end{center}

  Given two objects \(A, A'\) which both have \(\approx\) arrows to a third, \(A^+\); there
  exists an appropriate permutation to make the following diagram commute:
        
  \begin{center}
    \begin{tikzcd}
      A \ar[r, "p\approx q"] \ar[dr, "\sigma"] & A^+ \\
                                            & A'' \ar[u, swap, "r\approx q"]
    \end{tikzcd}
  \end{center}

  We shall call this the category of array shapes with transposes, reductions, and
  diagonals.
\end{definition}

The operation is here that we eliminate a duplicate index argument or dimension in our array's
domain cuboid. If we remember that we can extract a two-dimensional plane from
three-dimensional space by way of a planar equation, we get an idea as to what the semantics
may be.

If we then remember that the term `diagonal' is also used about relations to denote the
relation on a set given by equality
\(\Delta_S = \{ (x, x) \mid x \in S \} = \{ (x, y) \mid x, y \in S \land x = y \}\)
this also vaguely suggests what operation we might pursue.

Lastly, we remember that diagonal matrices have only non-zero entries on the diagonal,
and that the trace of a matrix is exactly the sum of the elements on the diagonal.

Hence in removing a duplicate array index, it seems prudent to interpret that as
the singular index in the new array specifying both of the duplicate indexes in the old array.
Elimination of duplicate indexes is taking the diagonal.

\section{Multiplication}

Just as division gave rise to interesting interpretations, so does multiplication.
Multiplication over the natural numbers is a total function, associative, commutative,
and defined recursively in terms of addition. We shall model two different cases of 
multiplication, the first being multiplication by a constant.

\begin{definition}
  \label{def:category-of-diagonals}
  Let \(\Arr\CADSSFR\) be a proper supercategory of \(\Arr\ADSSFR\).
  It has an extra schema of arrows as follows:

  Let \(A'\) be the product obtained by inserting the factor \(c\) into
  the product \(A\) at some position \(k\), meaning \(A = a_0 \times \cdots \times a_{d-1},\;
  A' = a_0\times\cdots\times c\times\cdots \times a_d\).

  There is a \(c\!\cdot\) arrow between \(A\) and \(A'\):
  \begin{center}
    \begin{tikzcd}
      A \ar[r, "c\cdot_k"] & A'
    \end{tikzcd}
  \end{center}

  This arrow is \emph{not} the inverse of the corresponding and opposite \(\div_k\) arrow.

  We shall call this the category of array shapes with transposes, reductions,
  diagonals, and tilings.
\end{definition}

We may now ponder how to interpret this. A good strategy is often to consider the simplest
possible interpretation: the simplest possible array would be one with unit domain,
a scalar \(A = [a]\) meaning \(\sharp A = [1]\)\footnote{There are of course infinitely
many unit-domain arrays, since for every \(d\) the cuboid \(\Nat_{<1}^d\) is a unit set; even
\(d=0\).}

To be free of constraints, we shall not consider what the domain set is. This leaves us with
only one possible interpretation: array tiling --- the replication of an array into additional
`layers'. Array tiling can be achieved by ignoring the
value of the newly created index, and just reporting the value of the old array using all
the other indexes, giving the `illusion' that there are multiple copies.

This poses an interesting composition left-inverse to
diagonal --- if we tile a number of times equal to a pre-existing factor,
the diagonal operation is a right-inverse to this tiling, and the following
diagram commutes:

\begin{center}
  \begin{tikzcd}
    & A' \ar[rd, "k\approx p"] & \\
    A \ar[rr, equal] \ar[ru, "(a_k)\cdot_p"] & & A
  \end{tikzcd}
\end{center}

There is another kind of multiplication which would be desirable to cover, but which is
not easily modelable since \(\Arr\ADSSFR\) is not a Cartesian category; meaning it does
not have an easy way to have ``multi-argument'' arrows. I am at present not even sure we can
consistently make actual products that have the universal property.

\begin{definition}
  \label{def:category-of-outer-products}
  Let \(\Arr\XCADSSFR\) be a proper supercategory of \(\Arr\CADSSFR\). It
  has an extra schema of arrows as follows.

  For two products \(A, B\), there is a pair of arrows \(\pi_A\) and \(\pi_B\)
  from the product \(A \times B\) to \(A\) and \(B\) respectively:
  \begin{center}
    \begin{tikzcd}
      A & A\times B \ar[l, "\pi_A", swap] \ar[r, "\pi_B"] & B
    \end{tikzcd}
  \end{center}

  We shall call this the category of array shapes with transposes, reductions,
  diagonals, array tilings, and outer products.
\end{definition}

The interpretation of \(\pi\) arrows is trickier, since they already bear a resemblance
to concatenation of \(\div\) arrows. As noted, this is an attempt at mimicking
the properties of the categorical product: on their own they signify nothing.

There is one operation which resembles the creation of one array from two, where
the result is of a rank which is the sum of the operands' ranks, is the outer product
in linear algebra: Two vectors of ranks \(n\) and \(m\) creates a matrix of shape \(n\times m\)
by pairwise multiplying the components in every combination.

\section{Including Domains, Future Work}

So far we have discussed only the shape of arrays. Of course there is more to
arrays than merely their shape; they also have a domain set over which their elements
range.

The very simplest way to begin modelling this based on what we have already defined,
is simply to use the Product Category construction.

\begin{definition}
  \label{def:cateogry-of-arrays-with-domains}
  Let \(\Arr \varnothing\) be a subcategory of the product category of
  the category of sets and \(\Arr\XCADSSFR\).

  The objects of \(\Arr\varnothing\) are pairs \((S, X)\) where \(S\) is a set,
  and \(X\) is a product; an object of \(\Arr\XCADSSFR\).

  The arrows of \(\Arr{\brm{Set}}\) are pairs \((f, t)\) where \(f\) is a function,
  and \(t\) is a transformation; an arrow of \(\Arr\XCADSSFR\), with the exception
  of the \(\pi\) and \(\div\) arrows.
  
  The \(\div\) arrows given in Definition~\ref{def:category-of-reductions} only exist
  if there is a Monoid structure on the set of the domain pair, and there exists one
  such arrow for each possible such Monoid.

  There exist product objects which obey the universal property. They are the objects
  of the form \((S \times T, A \times B)\) where \(S\times T\) is a product object in
  the category of sets, and \(A \times B\) is an object with the two arrows \(\pi_A, \pi_B\)
  as given in Definition~\ref{def:category-of-outer-products}. The projection arrows of
  \((S\times T, A\times B)\)  are \((\pi_1, \pi_A)\) and \((\pi_2, \pi_B)\).

  We call this the category of arrays with domains.
\end{definition}

This should nicely encapsulate the scope of information we wish to work with, and provide
a convenient embedding into the category given by the array functions themselves: map
the shape to the corresponding cuboid, and draw up a function with this cuboid as the
domain, and the set becomes the codomain.

We can now properly define the category of arrays.

\begin{definition}
  \label{def:category-of-arrays}
   
  Let \(\Arr{\brm{Set}}\) be a subcategory of the arrow category \(\mathrm{Arr}(\brm{Set})\)
  \footnote{Notational similarity is a coincidence.}, save for an extra family of morphisms.
  (Recall that an arrow category is given by its objects being the arrows of its parent category,
  and its arrows being commuting squares.)

  Every object in \(\Arr{\brm{Set}}\) is an array function as given by Definition~\ref{def:array}.

  The arrows of \(\Arr{\brm{Set}}\) are restricted to those consisting of postcomposition with
  functions
  \begin{center}
    \begin{tikzcd}
      \Nat_{<k}\dots \ar[r, "A"] \ar[d, "id"] & S \ar[d, "f"] \\
      \Nat_{<k}\dots \ar[r, swap, "f\circ A"] & T
    \end{tikzcd}
  \end{center}
  precomposition with functions from \(n\)-tuples to \(m\)-tuples
  that only re-orders, duplicates, or discards elements
  \begin{center}
    \begin{tikzcd}
      \Nat_{<k}\dots \ar[r, "A"] \ar[d, "g"] & S \ar[d, "id"] \\
      \Nat_{<k'}\dots \ar[r, swap, "A\circ g"] & T
    \end{tikzcd}
  \end{center}
  and the extra family takes arrays of elements of some set \(S\) to arrays
  of elements of the free monoid over \(S\), while discarding one dimension of
  the domain cuboid
  \begin{center}
    \begin{tikzcd}
      \Nat_{<k}\dots \ar[r, "A"{name=UPPER}] & S \\
      \Nat_{<k'}\dots \ar[r, "A'"{name=LOWER}] & S^*
      \ar[from=UPPER, to=LOWER, double, shorten <= 4pt, "list"]
    \end{tikzcd}.
  \end{center}
  
  This category is the true category of arrays.
\end{definition}

\begin{observation}
  \label{ob:category-of-arrays}
  Some observations on Definition~\ref{def:category-of-arrays}:
  \begin{itemize}
    \item In the category of arrays, the products are implicitly given. Since th domains
      of all arrays are (usually) Cartesian products already, an array with its codomain being
      a Cartesian product as well will readily be a product object from the definition of
      the arrow category on sets (in fact, several product objects
      at once, if the domain is a cuboid with more than two dimensions.) To compute the outer
      product, we postcompose with an appropriate function.

    \item The reduce operation is tantamount to postcomposition with a
      monoid morphism from the free monoid over a set, to another (possibly more useful) monoid.
  \end{itemize}
\end{observation}

A disclaimer: while we have made many definitions, these ultimately
serve to pump intuition on why the following set of array operations
are essential; to prove that these correspond to the appropriate arrows,
and obey the appropriate morphism properties is beyond the scope of this
project, and an avenue of future work would be to define the category of
arrays in a top-down manner.

\section{Array Operations}
\label{sec:array-operations}

\begin{remark}
  \label{rem:functions}
  Many useful operations can be derived from the function-nature of arrays. We touched
  on this in Definition \ref{def:flat}, where we precomposed an array with a monotone map
  from natural numbers to the domain of the array.

  In the following definitions we will touch upon less radical functions
  to compose with, and their meanings in terms of common operations.
\end{remark}

\begin{definition}
  \label{def:transpose}
  The transpose of an rank-\(d\)array \(A\) is the precomposition
  of the function corresponding to the array, with an \(d\)th degree permutation \(\sigma\).
  We denote the transpose with function composition \(A \circ \sigma\).
\end{definition}

\begin{example}
  \label{ex:transpose}
  The transpose known from linear algebra takes a rank-2 array, and
  applies the permutation \(\sigma = \left(\begin{smallmatrix} 1 & 2 \\ 2 & 1 \end{smallmatrix}\right)\).
\end{example}

\begin{definition}
  \label{def:map}
  The map of array \(A\) over a set \(S\) with a function \(f\) of the same domain,
  is an array of identical shape where each element is the function \(f\) applied to
  the corresponding element in the original array.

  This can be accomplished simply by postcomposing the function of \(A\)
  with \(f\), and we shall use no unusual notation, denoting it \(f \circ A\).
\end{definition}

\begin{example}
  \label{ex:scaling}
  The scaling operation in linear algebra, is to multiply every component in
  a vector by the same element of the underlying field.

  This can be modeled by the map operation. Thus in linear algebra \(k\brm v\)
  corresponds to \((x \mapsto kx) \circ \brm v\) in arrays. Here \(x \mapsto kx\) is
  the unnamed function that takes every number \(x\) to the product of \(k\) and \(x\).
\end{example}

\begin{definition}
  \label{def:reduce}
  The reduction of an rank-\(d\) array \(A\) over a set \(S\), on
  an axis \(k\) requires the specification of a monoid
  \((S, \oplus, \epsilon)\), and produces a rank-\((d-1)\) array \(B\), without
  axis number \(k\).

  It does so, by having each element in the reduced array \(B\), be the monoidal sum
  of the elements in axis \(d\) but otherwise at the same position in the original array \(A\).

  It follows the function definition:
  \begin{align*}
    B(x_0,\dots,x_{k-1},x_{k+1},\dots,x_{d-2}) = \bigoplus_{x_k} A(x_0,\dots,x_{k-1},x_k,x_{k+1},\dots,x_{d-1})
  \end{align*}

  We denote it by \( B = \reduce_{(S, \oplus, \epsilon),d} A \)
\end{definition}

\begin{observation}
  \label{ob:reduce} Some observations on Definition~\ref{def:reduce}:

  \begin{itemize}
    \item In a rank-1 array, the reduction is just the ordinary quantified monoid sum.
    \item If the array has null domain (see Definition~\ref{def:cuboid}) the result is
    the monoid identity.
  \end{itemize}
\end{observation}

\begin{example}
  \label{ex:reduce}
  The sum of the elements of each row-vector of a matrix can be found by using the monoid \((\Real, +, 0)\)
  and working on the second axis of a matrix (recall that axes are enumerated counting from zero)
  \begin{align*}
    \reduce_{(\Real, +, 0), 1} \begin{bmatrix}
      a & b & c \\
      d & e & f
    \end{bmatrix} = [(a + b + c)\;\;(d + e + f)]
  \end{align*}
  If we wish to sum the elements of each column-vector, we use the first axis
  \begin{align*}
    \reduce_{(\Real, +, 0), 0} \begin{bmatrix}
      a & b & c \\
      d & e & f
    \end{bmatrix} = [(a + d)\;(b + e)\;(c + f)]
  \end{align*}
\end{example}

\begin{remark}
  \label{rem:reduce}
  It is possible to use a simpler structure than a monoid to perform
  the reduce operation, for instance a Semigroup or an accumulator function. Using a semigroup
  makes the operation undefined for null arrays, unless we specify the neutral element
  on a case-by-case basis; using an accumulator function we lose the ability to do divide-and-conquer
  parallel processing and we are forced to introduce e.g. the left-fold/right-fold distinction.
\end{remark}

\begin{definition}
  \label{def:diagonal}
  Given an array \(A\) with two equal-sized axes \((\sharp A)(k_1) = (\sharp A)(k_2)\),
  the diagonal \(B\) elides the second axis, duplicates the coordinate given in the first
  axis to the second.

  Iterating over the remaining axis \(k_1\) in the diagonal array \(B\), will traverse the two relevant
  axes \(k_1,k_2\) in \(A\) in lockstep.

  It is given by the function definition
  \begin{align*}
    B(x_0, \dots, x_{k_1}, \dots, x_{k_2 - 1}, x_{k_2 + 1}, \dots, x_{d-1}) = \\
    A(x_0, \dots, x_{k_1}, \dots, x_{k_2 - 1}, x_{k_1}, x_{k_2 + 1}, \dots, x_{d-1})
  \end{align*}

  We denote it by \(B = \diag_{k_1,k_2} A\).

  If more than two axes are equal, this definition of binary diagonal can easily be extended to
  \(n\)-ary diagonal by repeated application of the operator.
\end{definition}

\begin{example}
  \label{ex:trace}
  The trace of a matrix is the sum of the main diagonal. We can describe it with the diagonal
  and reduce operators
  \begin{align*}
    \brm A &= \begin{bmatrix}
      a_{0,0} & \cdots & a_{0,n{-}1} \\
      \vdots & \ddots & \vdots \\
      a_{n{-}1,0} & \cdots & a_{n{-}1,n{-}1} \\
    \end{bmatrix} \\
    \operatorname*{\brm{tr}} \brm A &= \reduce_{(\Real,+,0),0} \diag_{0,1} \brm A \\
    \operatorname*{\brm{tr}} \brm A &= \reduce_{(\Real,+,0),0} [ a_{0,0}\;a_{1,1}\;\cdots\; a_{n{-}1,n{-}1} ] \\
    \operatorname*{\brm{tr}} \brm A &= a_{0,0}+a_{1,1}+\cdots+ a_{n{-}1,n{-}1} 
  \end{align*}
\end{example}

\begin{definition}
  \label{def:tiling}
  The tiling of an rank-\(d\) array \(A\) adds a new axis \(d\) with maximum \(k\)
  which is ignored in the indexing operation, producing an rank-\((d+1)\) array \(B\).

  It is given by the function definition
  \begin{align*}
    B(x_0, \dots, x_{d-1}, x_d, x_{d+1}, \dots, x_{d}) = A(x_0, \dots, x_{d-1}, x_{d+1}, \dots, x_d)
  \end{align*}

  We denote it by \(B = \tile_{\times d, k} A\)
\end{definition}

\begin{definition}
  \label{def:outer-product}
  The outer product combines two arrays \(A\) and \(B\) over
  two potentially different sets \(S\) and \(R\), combines
  them combinatorially with a function \(f\) that takes two
  arguments from \(S\) and \(R\).

  The resulting array \(C\) has a shape which is the concatenation
  of the shapes of \(A\) and \(B\). It is given by the function definition
  \begin{align*}
    C(x_0,\dots,x_{d-1},x_d,\dots,x_{g-1}) = f(A(x_0,\dots,x_{d-1}), B(x_d,\dots,x_{g-1}))
  \end{align*}

  We denote the outer product of two arrays \(A\) and \(B\) by way of the combining
  function \(f\), with the notation \(A \oprodby f B\).
\end{definition}

\begin{remark}
  Contrary to the category-theoretical definition of the outer product given in
  Definition~\ref{def:category-of-arrays}, the outer product as defined in Definition~\ref{def:outer-product}
  is predicated on some combination function. To achieve the most general outer product,
  we simply use the pairing function \(P(x, y) = (x, y)\).
\end{remark}

\begin{example}
  \label{ex:matrix-product}
  The matrix product can be derived with the outer product,
  the diagonal, and reduction.

  The matrix product takes two arrays of such a shape that the
  second axis of the first is equal to the first axis of the second
  \(\sharp A = [k\; m], \sharp B = [m\; n]\). It then discards those
  two axes, and produces and array of shape \(\sharp X = [k\; n]\).

  The matrix product also includes the operations multiplication and
  summation, taken from the inner product. The inner product gives us
  the clue that multiplication is the combining operation.

  Consider the outer product \(A \oprodby\times B\). It has the
  shape \([k\;m\;m\;n]\). This not only contains the undesired middle axis,
  but does so in duplicate.

  We also know from the inner product that summation is involved, a reduction
  by the addition monoid. We cannot reduce on two axes at once, so we eliminate
  the duplication bu taking the diagonal
  \begin{align*}
    \sharp \diag_{1,2} A \oprodby\times B = [k\;m\;n]
  \end{align*}

  Finally, reducing the middle axis away gives us the desired shape
  \begin{align*}
  \sharp \reduce_{+,1} \diag_{1,2} A \oprodby\times B = [k\;n]
  \end{align*}

  Writing out the function definition, we see that it is indeed the matrix product
  \begin{align*}
    \left(\reduce_{+,1} \diag_{1,2} A \oprodby\times B\right) (i, k)
    &= \sum_{j} \left(\diag_{1,2} A \oprodby\times B\right) (i, j, k) \\
    &= \sum_{j} \left(A \oprodby\times B\right) (i, j, j, k) \\
    &= \sum_{j} A(i, j) \times B(j, k) \\
  \end{align*}
  showing our array formalism is strong enough to decompose operations
  otherwise thought of as fundamental.
\end{example}

\chapter{Towards Practicality}

\section{Practical Array Functions}

To implement a multidimensional array, one constructs an algorithm for changing the coordinates
of the array's domain cuboid into indexes into a flat array. This is usually done by interpreting
the entries in the tuple as digits in a mixed radix number, and then doing a base conversion to
machine integers which can be used to index the flat array.

This operation is available in very limited capacity as an instruction on some RISC processors,
such as the X86 family. Many memory-indexing operations in X86 assembly allow the computation of
an array index using a dynamic base pointer, a static offset, a dynamic index number, and a static
element size.

The \texttt{lea} instruction is a pure implementation of this index calculation:
\texttt{lea edi, [ebx + esi * \textit N + \textit k]}. Here \texttt{edi} is the register
that holds the computed pointer, \texttt{ebx} holds the pointer to the array structure,
\texttt{\textit k} signifies the offset from the header of the array structure to the actual
array body (often this is 0). The \texttt{\textit N }value
is one of 1, 2, 4, and 8 on X86-64 platforms, and is the byte width of the array elements, while
\texttt{esi} here holds the desired index into the array.

In computing the flat-array index from a multidimensional array index, we generalizes the concept from LEA,
first by ignoring the constant offset \texttt{\textit k} and then by leaving the base pointer and
element size implicit. 

To actually compute the flat-array index, we use a mixed radix computation; meaning that the least
significant coordinate is multiplied by 1, the second-to-least significant coordinate is multiplied
by the upper bound of the least significant coordinate, the third-to-least is multiplied by the product
of the upper bound of the two former dimensions, and so on. This is exactly like how in base-10 we multiply
the least significant digit by 1, the second-to-least by \(10 = 1\times 10\), the next one by
\(100 = 1\times 10\times 10\). 

One complication is that we in our definition of the flat traversal \(\flat A\) prefer the last index
to be the least significant. Index computation in a cuboid \(\Nat_{<k_0} \times \cdots \times \Nat_{<k_{d-1}}\) is
thus given by the function
\begin{align*}
  J(x_0, \dots, x_{d-1}) &= \sum_{i=0}^{d-1} \left( x_i \prod_{j=i+1}^{d-1} k_j \right) \\
  &= x_{d-1} + x_{d-2} k_{d-1} + x_{d-3} k_{d-2} k_{d-1} + \cdots + x_0 k_1 \dots k_{d-1} \\
  &= x_0 k_1 \dots k_{d-1} + x_1 k_2 \dots k_{d-1} + \cdots + x_{d-2} k_{d-1} + x_{d-1} \;.
\end{align*}

Notice that this is a linear function given by the scalar product of the vector of coordinates
\(\brm x = [x_0\; \dots\; x_{d-1}]\) by the vector of radixes \(\brm r\) given by
\begin{align*}
  \brm r_i &= k_{i+1} \brm r_{i+1} \\
  \brm r_{d-1} &= 1 \;.
\end{align*}

We also observe that \(I\) is itself an array, with its codomain being the natural numbers. As an additional
bonus, we have that the flat traversal is just successive natural numbers in sequence: \((\flat I)(i) = i\).
This is a very desirable property, because we can now decompose our arrays into a flat array which is easily
representable in memory, and a linear function which is easily computable:
\begin{theorem}
  \label{the:linear-decompose}
  Every array \(A : \Nat_{<k_0} \times \cdots \times \Nat_{<k_{d-1}} \to S\) can be written as
  the composition of a linear function \(J\) and its own flat traversal:
  \begin{align*}
    A = \flat A \circ J
  \end{align*}
  
  We shall call the linear function the flat radix.
\end{theorem}
\begin{proof}
  As outlined in the section above.
\end{proof}

\section{Preservation of Origin Index}

The array operations outlined in Section~\ref{sec:array-operations} all have the
property that they preserve the value at the origo coordinate index, by way of function
application:
\begin{align*}
  (\operatorname{op} A)(0, 0,\dots,0) &= f(A(0, 0, \dots, 0)) && .
\end{align*}
For most of the listed operations, save reduce and outer product, the function in question is the
identity function. For the reduce operation, if we employ the Free Monoid, as in
Definition~\ref{def:category-of-arrays}, then the origo will be the first element of the list.
Similarly, if we use the identity function as the combining function in the outer product operation,
yielding an array of pairs as in the product objects also in Definition~\ref{def:category-of-arrays},
then the origo of the outer product is a pair of the origo elements of the constituent arrays.

This distinction --- that operations preserve origo --- is significant, because it leads us to
consider operations that do not preserve the property of origo-as-function-of-origo: most significantly
slicing, and various re-orderings, which we have not covered so far.

It is interesting to notice that this dichotomy between origo-preserving and not, is also present
in geometry, in the difference between a linear and affine transformation. In fact the two
are quite readily connected.

By Theorem~\ref{the:linear-decompose}, we even see this relationship with linear algebra directly,
and it suggests to us lines of inquiry: what if the flat radix function is \emph{not} a mixed radix
conversion? What if the flat radix function is instead affine?

Given the array shape, \(\sharp A = [a\; b\; c]\) we know that \(A\) has the flat radix given
by the covector\footnote{Every covector gives rise to a linear functional by the scalar product.} \([bc\; c\; 1]\).
If we transpose \(A\) by swapping the two last dimensions
\[A' = A \circ \left(\begin{smallmatrix}1&2&3\\1&3&2\end{smallmatrix}\right),\quad \sharp A' = [a\; c\; b]\]
it turns out that we can specify \(A'\) in terms of \(\flat A\) like so:
\[A' = (\brm x \mapsto [bc\; 1\; c]\cdot\brm x) \circ \flat A\quad.\]
Any time we apply a transpose to an array, we can represent it as that same transpose only applied
to the linear map from the coordinate cuboid of the array to the flat representation; leaving the flat
representation intact.

This property extends in a number of interesting ways to both repetition and diagonals:
given an array \(A\) with shape \(\sharp A = [a\; b\; b]\) it has the flat radix \([b^2\;b\; 1]\).
Applying the diagonal operation on the like axes \(A' = \diag_{1,2} A\) yields an array of the shape
\(\sharp A' = [a\; b]\) and is expressible in terms of \(\flat A\) by the covector \([(b^2 + b)\; 1]\).
Given the same array and the tiling operation \(A' = \tile_{\times c, 3} A\) yields an array
of shape \(\sharp A' = [a\; b\; b\; c]\) and is expressible in terms of \(\flat A\) by the covector
\([b^2\; b\; 1\; 0]\).

In essence, in the covector translation from cuboid coordinates to flat index, we see that
transposes map to permutation of the components, diagonals map to addition of components,
and tilings map to insertion of zeros.

This, astute readers might recognize as operations which are easily modelable by left-multiplying
our mixed-radix base covectors to various matrices
multiplication: permuting covectors is done with permutation matrices, adding up columns
for the diagonal operation can be done by multiplying by a \(d \times (d-1)\) matrix where
one col

\section{Affine Transformations}

As we have seen, there is a correspondence between operations on linear functions' covector representations
and the array operations outlined; but we are still short of one kind of very common operation: the slice.

Slicing an array can trivially fail to preserve the value-at-origin, meaning it does not uphold this invariant
that all the other operations have; hence it was not included in the category theory section --- it is very practical
but of marginal theoretical significance.

It is also easily implemented, when we remember that all linear functions are affine functions too. An affine function \(f'\)
is given by a linear function \(f\), as well as a constant \(c\). When the function value of a vector, one applies
the linear function as normal, then adds the constant:
\[ f'(\brm x) = f(\brm x) + c \]

Let us consider the trivial example of a flat array, which decomposes into itself and the one dimensional linear
identity function.
\begin{align*}
  A &= \flat A \\
  A &= \flat A \circ \mrm{id}_{\Nat^1} \\&= A \circ \mrm{id}_{\Nat^1}
\end{align*}
In order to slice \(A\), we might exchange the linear function \(\mrm{id}_{\Nat^1}\) with a function of the form
\[ f(\brm x) = [1] \cdot \brm x + 1 \]
which would yield an array \(A' = \flat A \circ f\) with the property that the \(n\)-th element of \(A'\) is
equal to the \((n+1)\)-th element of \(A\) like so
\[ A'(n) = A(n+1) \]
which is essentially the `tail' operation known from most functional programming languages.

Similarly we can also implement 'stride' by deviating from the strict requirement that the linear function
in question be a mixed radix base conversion subjected to multiplication by permutation matrices. For a given
flat array \(A = \flat A\) we can take every second entry by composing with the function \(f(\brm x) = 2\brm x\).

\section{Homogenous Coordinates in Earnest}

The idea of using affine functions to manipulate arrays without having to re-create the underlying flat
representation in memory, can extend to more general arrays as well. For an arbitrary array \(A\) we can
achieve much the same result --- the ability to represent tiling, diagonals, transposes, and slices ---
by precomposing it with an affine function represented by a matrix in homogenous coordinates:
\[ A = A \circ \brm I \]
In the case of \(A\) being flat, this reduces to the above case of an affine function.
We can represent transposes by re-ordering the rows of the matrix, diagonals by adding them, tilings by
inserting new zero rows, and slices by scaling rows and adding multiples the final row of the matrix (zero
on all coordinates save the homogenous one.)

This is exactly the set of matrix row operations (save perhaps the insertion of new rows --- but note that
this preserves the span of the row vectors.)

Matrices are tried and true, easy to implement, and will form the basis of the copy-less slicing of my library.

\chapter{Implementation}

Code is available from the git repository
\begin{center}
\url{https://github.com/Karl-D-Asmussen/BachelorThesis/}.
\end{center}
In the sub-directory \texttt{cat\_arrays}, you will find a Cargo project;
source files are in the \texttt{cat\_arrays/src} directory.

\section{Rust}

For the implementation I have chosen the Rust programming language. This
is for several reasons.

Rust is developed by Mozilla, which first appeared in early beta in 2010. It
builds on many modern ideas of programming-language design, and have benefited
greatly from a duration of open beta development where the community could submit
proposals styled after the IETF\footnote{Internet Engineering Task Force}
RFC process. This has led to the 1.0 release having a solid set of standard libraries,
as well as a very ergonomic syntax.

Core to Rust's usefulness is that it is semantically an ML-family language, with a strong type
system based on `Traits' which mimic Haskell's type classes. Further, it imposes an additional
inference system based on the idea of attaching abstract lifetimes to stack frames, which allows
the compiler to reason thoroughly about ownership of allocated resources.

Syntactically, Rust resembles Scala and Apple's Swift, but has no garbage collector; instead relying
on the lifetime inference (``borrow checker'') to decide when resources can be safely freed. This not
only allows Rust to have similar performance to, and compete with C/C++, but the strength of the lifetime
inference system allows almost complete elimination of data races. This makes Rust the only language
where concurrency is not only encouraged, but in fact easy.

Rust therefore provides the performance expected of a data-processing library, but also
provides a novel paradigm of library design using the borrow checker. The borrow checker provides
statically enforced guarantees against dangling pointers, which eliminates the problems
of data sharing.

For instance, UTF-8 strings are handled in Rust via two types: \texttt{String} and \texttt{\&str}.
\texttt{String} is a heap-allocated vector of UTF-8 data, which is growable and is freed upon destruction.
Meanwhile \texttt{\&str} is a non-allocating data slice, which refers to a section of a \texttt{String}.
The borrow checker then statically ensures that a \texttt{\&str} never points to deallocated memory, and
the programmer can pass around immutable slices and never worry about data races or data duplication\footnote{
This design repeats with generic growable arrays and array slices, and many other data structures.}.

\section{Design Overview}

I've chosen to call my library \texttt{cat\_array} for ``Categorical Arrays.''  It is made with the
\texttt{cargo} project manager, and is available on GitHub.
It is meant to showcase the power of the operations listed in Section~\ref{sec:array-operations},
and isn't intended to be a realistic contender against more optimized code.

However, in using Rust, we gain the benefit of Rust's main selling point: zero-cost abstractions.
Rust aims to encroach on the niche which C++ fills in providing the systems programming capability
of C together with high-level abstractions. In C++'s case, templates and object orientation; in the
case of Rust, a Hindley-Milner type system and static memory and data-race safety enforced through
linear types.

Rust uses the concept of a trait which is used both as constraints in parametric polymorphism,to
implement ad-hoc polymorphism, and also to implement dynamic dispatch polymorphism. Using this feature
to define the `core' functionality, rather than deciding on a central type beforehand, allows for
a more organic development process and to closer approximate the forgiving nature of e.g. NumPy's
typing discipline.

The core functionality is described by the traits \texttt{ArrayLike}, \texttt{ArrayLikeMut}, and
the \texttt{Into*} trait family. They describe the operations of shape (\(\sharp\)), flat (\(\flat\)),
reshape (\(\varrho\)), transpose (\(A \circ \sigma\)), diagonal (\(\diag\)), and tiling (\(\tile\))
through the appropriately named trait-associated types and member functions.

Notably, every operation has its own return-type specified in the trait; which allows for very accurate
description of array functionality. Unfortunately, many of these operations in their most general forms
rely on difficult-to-describe data structures (permutations for the transpose, notably) and are thus
implemented to allow only the `simplest' forms (transpose can only swap two dimensions, tiling always
makes a new most-significant index, etc.)

The existence of three traits (actually, two traits and a family of traits) exists to take full advantage
of the structure of Rust's ownership semantics: owned, referenced, and mutably referenced values all
allow for different operations (notably, \texttt{ArrayLikeMut} does not allow tiling as it is uncertain what
assigning to a single entry in a mutable tiled array should do to the underlying array.) To alleviate
this impediment, we get the \texttt{IntoTranspose} trait and others like it, allowing the composition
of `swap' permutations without new memory allocation in types that support it.

There is also the matter of the reduce (\(\reduce\)), map, and outer-product operations which are not
represented. Whereas these might be easy to represent in a marginally more expressive language like
Haskell, it just so happens that Rust entirely lacks higher-kinded polymorphism. Type variables can
only take on the values of concrete types; not type constructors\footnote{This in turn becomes troublesome
when one wishes to make full use of the explicit lifetimes of stack-allocated data which Rust incorporates
into the ownership model.}.

Map, reduce, and outer-product are thus instead supplied through their own concrete types, implemented
in a lazy manner, allowing the programmer to compute only the sections of an array they need and also
determine whether it is necessary to compute the full array.

\section{Bootstrapping}

The idea of multidimensional arrays is comprehensive, and require a lot of interconnected functionality.
For instance, just the very act of indexing an array \(A(x_1, x_2, \dots, x_d)\) requires a way to represent
the indexing vector \(\brm x = [x_1\; x_2\; \dots\;x_d]\) as an array itself.

This leads to an incremental design centered around the \texttt{DummyArray} type and its implementation of
the two core traits. The dummy type implements all functionality using the \texttt{unimplemented!()} macro
in Rust, causing every array-related call to cause an execution-thread panic.

Similarly, the dummy array type allows `stubbing out' the associated types which have not yet been determined.

In this vein, several `simple' array types are given, developed in an interconnected manner: \texttt{Unit},
\texttt{Const}, and an instance of array functionality is defined for every standard-library type that implements slicing
(including but not limited to: string types, heap-allocated arrays, growable vectors, and slices.) Only then does
it become viable to start implementing more advanced array types.

The necessity of such simple array types is to fulfill the requirement of the shape and indexing operation: the
shape of an array is always a rank-1 array, and the shape of a rank-1 array is always a singleton; similarly
the indexing operation takes as an argument an array of coordinate components. With just these simple types defined
it is already possible to hook arrays into the extensive \texttt{Iterator} functionality provided by the Rust
standard library.

\section{Overview of the ArrayLike traits}

\textit{Warning:} This section will be heavy in Rust code; a passing familiarity is expected, and can
be gained from the official documentation\cite{RDOC117}.

The \texttt{ArrayLike} trait requires several functions, which can be roughly divided into two categories.
The member functions that `inspect' the array type:

\begin{lstlisting}
fn rank(&self) -> usize;

fn len(&self) -> usize;

type Entry;
fn get<I>(&self, coord : &I) -> &Self::Entry
  where I : ArrayLike<Entry = isize>;

type Shape : ArrayLike<Entry = usize>;
fn shape(&self) -> &Self::Shape;
\end{lstlisting}

And the member functions that create new arrays:

\begin{lstlisting}
type Flat : ArrayLike<Entry = Self::Entry>;
fn flat(&self) -> Self::Flat;

type Reshape : ArrayLike<Entry = Self::Entry>;
fn reshape<I>(&self, shape : &I) -> Self::Reshape
  where I : ArrayLike<Entry = usize>;

type Slice : ArrayLike<Entry = Self::Entry>;
fn slice<I>(&self, cuboid : &I) -> Self::Slice
  where I : ArrayLike<Entry = (isize, Option<isize>, isize)>;

type Fixate : ArrayLike<Entry = Self::Entry>;
fn fixate(&self, ax : usize, at : isize) -> Self::Fixate;

type Transpose : ArrayLike<Entry = Self::Entry>;
fn transpose(&self, ax1 : usize, ax2 : usize) -> Self::Transpose;

type Reverse : ArrayLike<Entry = Self::Entry>;
fn reverse(&self, ax : usize) -> Self::Reverse;

type Diagonal : ArrayLike<Entry = Self::Entry>;
fn diagonal(&self, ax1 : usize, ax2 : usize) -> Self::Diagonal;

type Tile : ArrayLike<Entry = Self::Entry>;
fn tile(&self, len : usize) -> Self::Tile;
\end{lstlisting}

Notice here the pattern
\begin{lstlisting}
fn func<I>(...) where I : ArrayLike<Entry = ...>
\end{lstlisting}
which denotes that the function can take as argument any array type so long
as it contains the correct data type (often subject to runtime checks, since rank
and shape cannot be determined statically.)

There are two particular functions we have neglected to mention, which is the reversal of an array
over a given axis, and the other is the elimination of an axis by fixing it to a pre-set index
as mentioned in the discussion following Definition~\ref{def:category-of-reductions}. These two
operations are called \texttt{Reverse} and \texttt{Fixate} respectively, and are relatively uncontroversial.

The fixate operation is essentially the concept of currying, applied to arrays.
The distinction between fixating and slicing is that slicing cannot wholly eliminate an axis of
an array; only reduce it to an upper bound of 1. Fixation does the same, but additionally gets rid
of the axis entirely.

The \texttt{ArrayLike::shape} function is interesting in that it is the only array-returning function in the
trait that does not return an array, but a reference to an array internal to the parent array type.
Notice that lifetime elision\footnote{\cite[chapter~3.4]{RNOM117}} is at play, and the full type signature
for \texttt{shape} is:
\begin{lstlisting}
fn shape<'a>(&'a self) -> &'a Self::Shape;
\end{lstlisting}
making it clear that the shape of an array should be available as internal data of the array.

The other array-returning functions all return actual array types, intended to be wrappers around
the reference to \texttt{self} to avoid copying.

The counterpart to \texttt{ArrayLike} is of course \texttt{ArrayLikeMut}, which covers many
of the same operations, but in a mutable manner. It is based on the \texttt{set} function which
mirrors \texttt{ArrayLike::get}:
\begin{lstlisting}
fn set<I>(&mut self, coord : &I, val : <Self as ArrayLike>::Entry) -> &mut Self
  where I : ArrayLike<Entry = isize>;
\end{lstlisting}
Notice that we return the mutable reference to \texttt{self} at the end of the call,
allowing for classic method chaining. Notice also how Rust avoids the problem of trait-member
namespace pollution by the \texttt{<Type as Trait>::member} syntax.

The operations supported in \texttt{ArrayLikeMut} are the same as in \texttt{ArrayLike},
save for the exclusion of the \texttt{ArrayLike::Tile}:
\begin{lstlisting}
  type FlatMut : ArrayLike<Entry = <Self as ArrayLike>::Entry> + ArrayLikeMut;
  fn flat_mut(&mut self) -> Self::FlatMut;

  type ReshapeMut : ArrayLike<Entry = <Self as ArrayLike>::Entry> + ArrayLikeMut;
  fn reshape_mut<I>(&mut self, shape : &I) -> Self::ReshapeMut
    where I : ArrayLike<Entry = usize>;

  type SliceMut : ArrayLike<Entry = <Self as ArrayLike>::Entry> + ArrayLikeMut;
  fn slice_mut<I>(&mut self, cuboid : &I) -> Self::SliceMut
    where I : ArrayLike<Entry = (isize, Option<isize>, isize)>;

  type FixateMut : ArrayLike<Entry = Self::Entry> + ArrayLikeMut;
  fn fixate_mut(&mut self, ax : usize, at : isize) -> Self::FixateMut;

  type TransposeMut : ArrayLike<Entry = <Self as ArrayLike>::Entry> + ArrayLikeMut;
  fn transpose_mut(&mut self, ax1 : usize, ax2 : usize) -> Self::TransposeMut;

  type ReverseMut : ArrayLike<Entry = <Self as ArrayLike>::Entry> + ArrayLikeMut;
  fn reverse_mut(&mut self, ax : usize) -> Self::ReverseMut;
  
  type DiagonalMut : ArrayLike<Entry = <Self as ArrayLike>::Entry> + ArrayLikeMut;
  fn diagonal_mut(&mut self, ax1 : usize, ax2 : usize) -> Self::DiagonalMut;
\end{lstlisting}
All of the \texttt{*Mut} types implement both \texttt{ArrayLike} and \texttt{ArrayLikeMut},
and are intended to allow things like assigning to certain subsections of an array.

Last there is the \texttt{Into*} family of traits which all have the structure:
\begin{lstlisting}
pub trait IntoFlat : ArrayLike {
  type To : ArrayLike<Entry = <Self as ArrayLike>::Entry>;
  fn into_flat(self) -> Self::To;
}
\end{lstlisting}
Notice here that the \texttt{IntoFlat::into\_flat} function does not capture \texttt{self} by
reference, but by \textit{value}. This means the function takes ownership of \texttt{self} and
all responsibility of running the deconstructor.

Traits like this will be particularly useful for a generalized reshape wrapper and generalized slice wrapper
types, with which we would like to avoid nesting unnecessarily, since they naturally compose.


\section{Main ArrayLike types}

There are several `main' types the library is centered around.
\begin{description}
  \item[\texttt{DummyArray<T>}] a white-hole implementation. Defined in \texttt{dummy.rs}.
  \item[\texttt{Unit<T>}] array of shape \([1]\). Defined in \texttt{unit.rs}.
  \item[\texttt{Const<T>}] constant functions of any shape. Defined in \texttt{const.rs}.
  \item[\texttt{Map<'a, F, T, U, A>}] lazy map operation. Defined in \texttt{map.rs}.
  \item[\texttt{Fold<'a, F, T, U, A>}] lazy fold operation (in lieu of proper reduce.) Defined in \texttt{map.rs}.
  \item[\texttt{OuterProd<'a, T, U, V, A, B>}] lazy outer-product operation. Defined in \texttt{outer\_prod.rs}.
  \item[\texttt{GenReshape<T, A>}] generic radix-conversion-based wrapper to reshape any array. Defined in \texttt{gen\_reshape.rs}.
  \item[\texttt{GenSlice<T, A>}] generic homogenous-coordinate matrix-based wrapper to slice any array. Defined in \texttt{gen\_slice.rs}.
\end{description}

There is also a generic implementation for the built in arrays and slices, accomplished through
implementing \texttt{ArrayLike} for every type that has the trait \texttt{Deref<Target = [T]>}.
\texttt{Deref} is an operator-overloading trait, that allows for ``pointer-like'' traits to
work --- this is used extensively in Rust to implement smart-pointers and array-like types such
as growable arrays.

The types \texttt{Unit}, \texttt{Const}, \texttt{GenReshape}, and \texttt{GenSlice} all come in
three varieties: basic, -\texttt{Ref}, and -\texttt{MutRef} signifying the concepts of owned types,
references, and mutable references inherent in Rust. \texttt{Unit} owns its single value and will
for most operations  perform a \texttt{clone} operation that has the potential to do heap allocation.
\texttt{UnitRef} however, contains a reference, and references are essentially pointers and therefore
bit-copyable. \texttt{UnitMutRef} also implements \texttt{ArrayLikeMut}.

The types \texttt{Const} and \texttt{GenReshape}/\texttt{GenSlice} all contain a few heap-allocated
structures to designate e.g. their shape, but the amount is negligible.

The \texttt{GenReshape}/\texttt{GenSlice} types are designed to work with almost any type, and
provide a `quick fix' way of implementing \texttt{ArrayLike}

\section{Lack of HKT, Excessive use of \texttt{Clone}}

Rust lacks higher-kinded types. It is in the development pipeline to one day support both higher-kinded
types and \(\Pi\)-types, but at present, both of these are a detriment to the efforts of this implementation.

In particular, a common conflict is the following: given a pair of types such as \texttt{Unit<T>} and
\texttt{UnitRef<'a, T>} we find ourselves in want of a way to implement e.g. \texttt{ArrayLike::flat}
in such a manner that it returns the reference type. This is, however, impossible.

The \texttt{flat} operation has (and indeed it is difficult to conceive of how else to implement it) the
signature:

\begin{lstlisting}
  fn flat(&self) -> Self::Flat;
\end{lstlisting}

This hides the lifetime of the reference on \texttt{self} by way of lifetime elision:

\begin{lstlisting}
  fn flat<'a>(&'a self) -> Self::Flat;
\end{lstlisting}

So, what we wish to do is supply the associated type \texttt{ArrayLike::Flat} as \texttt{UnitRef} like
so:

\begin{lstlisting}
  type Flat = UnitRef<'a, Self::Entry>;
  fn flat<'a>(&'a self) -> Self::Flat;
\end{lstlisting}

But here the problem arises: the lifetime \texttt{'a} is not in scope at the point where we declare
\texttt{Self::Flat}. Here it \emph{would} be prudent to supply an associated type \emph{constructor}
rather than merely a type:

\begin{lstlisting}
  type Flat<'a> = UnitRef<'a, Self::Entry>;
  fn flat<'a>(&'a self) -> Self::Flat<'a>;
\end{lstlisting}

But alas, this is impossible. Should it some day become possible, this entire library could be
rewritten to become potentially many times more efficient.

It is therefore recommended to eliminate copying by using the \texttt{*Ref<'a, ...>} family of
types whenever possible --- these only perform a pointer-copy of the reference to the underlying
array.

\section{Returning References in Lazy Arrays, Why a Referencing Index Operation was a Mistake}

The problem with lazy types is that the \texttt{ArrayLike::get} method is required to return
a reference type. Computing the desired result within the function body and attempting to return
a reference to it is disallowed by the Rust compiler, as the required data is almost always allocated
on the called function's stack frame --- a pointer thereto would be rendered dangling as soon as the
function returned.

This is an oversight of design and I am suspect of its correctness especially in \texttt{Map::get} which
is implemented as:
\begin{lstlisting}
impl<'a, F, A : 'a, T, U> ArrayLike for Map<'a, F, A, T, U>
where A : ArrayLike<Entry = T>, F : FnMut(T) -> U {
  type Entry = U;
  fn get<I>(&self, coord : &I) -> &Self::Entry
  where I : ArrayLike<Entry = isize> {
    get_set_coord_check!(Map::get, self, coord);
    &self.0(self.1.get(coord))
  }
}
\end{lstlisting}
On line 4 of this snippet, the tuple indexing \texttt{self.0} has a type implementing
the function trait \texttt{Fn(\&'a T) -> U} where \texttt{'a, T, U} are generic lifetime
and type parameters given by \texttt{Map}. However, the function \texttt{self.0} returns
a \texttt{U}, which we immediately reference, leaving ownership in `limbo.' So to speak.
The compiler does not complain, but as I've said, I am suspect.

More egregious is the \texttt{Fold} type, which in its current form is an unsafe hack.
Out of ignorance, my current implementation uses the concept of interior mutability\cite[chapter 3.11]{RPL117}.
In the implementation of \texttt{Fold::get} I resort to using unsafe casting primitives to
circumvent the problems imposed by restrictive lifetimes.
\begin{lstlisting}
  fn get<I>(&self, coord : &I) -> &Self::Entry
  where I : ArrayLike<Entry = isize> { 
    let mut ix : Vec<_> = self.shape().iter().map(|a| *a as isize).collect();
    ix.insert(self.3 .0 as usize, 0);
    let mut acc = self.2.clone();
    for i in 0 .. *self.0.shape().get(&self.3) {
        ix[self.3 .0 as usize] = i as isize;
        acc = self.1(acc, self.0.get(&ix))
    }
    { *self.4.borrow_mut() = Some(acc) }
    match unsafe { transmute(&*self.4.borrow()) } {
        &Some(ref q) => return q,
        &None => unreachable!()
    }
  }
\end{lstlisting}
The field \texttt{self.4} is a \texttt{RefCell<T>} type: a primitive used in rust's standard
library to allow data structures to have internally used mutable data while appearing 
outwardly immutable (e.g. the reference counted pointer type \texttt{Rc<T>} uses this
concept internally.)

At the time of implementing \texttt{Fold} I thought it possible to use a single cell
of memory to allow the computation of the result value and then return that. This of
course creates a massive problem when one wishes to e.g. add two cells of a \texttt{Fold}
together, as the computation of the second value will invalidate the reference to the
first.

This issues have come up in code review, and introduces unsoundness into the program. 
Rust is predicated on strong soundness guarantees: no data is mutably and immutably
referenced at the same time, no immutably referenced data mutates, no mutably referenced
data has more than one reference to it.

\section{Falling Short of the Goal, Plan for v.0.2, Future Work}

\begin{center}
  ``Everything should be built top-down, except the first time.''\\
  --- \emph{Alan Perlis}\cite{AP82}.
\end{center}

As stated in the abstract and introduction, the goals I set for this project
back in January was to eventually arrive at a practical array library capable
of performing linear algebra operations as per Example~\ref{ex:matrix-product}.

What I have produced, as can be found in the theoretical sections, and the partial
implementation of my intended library in Appendix~\ref{sec:apx-code}.

A brief listing of what work is left undone:
\begin{itemize}
  \item A top-down realization of \(\Arr{\brm{Set}}\) with accompanying
    proofs of category-theoretically significant properties.
  \item An implementation of the outer-product array type.
  \item An implementation of the general slice array type.
  \item Rewriting several hacks to idiomatic rust code.
  \item Re-thinking of the trait structure.
  \item Finding a way to circumvent lifetime problems.
  \item Implementing linear algebra algorithms.
\end{itemize}

In particular, there is a technique to circumvent the absence of
higher kinded types in Rust, which is to implement a trait\cite{HKT15}:
\begin{lstlisting}
trait HKT<T> {
  type Content;
  type Replaced;
}
\end{lstlisting}
and implement it for types with a generic type parameter:
\begin{lstlisting}
impl<T, U> HKT<U> for Vec<T> {
  type Current = T;
  type Replaced = Vec<U>;
}
\end{lstlisting}

This effectively enables us to use \texttt{HKT<U>} as a trait bound
to implement e.g. a trait that mimics the \texttt{Functor} type class in Haskell.

A similar idea can be applied to lifetimes:
\begin{lstlisting}
trait LTP {
    type _Ref : Deref<Target = ()>;
}

impl<'a, T> LTP for &'a T {
    type _Ref = &'a ();
}

impl<'a, T : ToOwned> LTP for &'a mut T {
    type _Ref = &'a ();
}

trait LTPG<'a> : LTP {
    type Current;
    type Replaced;
}

impl<'a, 'b, T : 'a + 'b> LTPG<'a> for &'b T where Self : LTP<_Ref = &'b ()> {
    type Current = &'a T;
    type Replaced = &'b T;
}

impl<'a, 'b, T : 'a + 'b> LTPG<'a> for &'b mut T where Self : LTP<_Ref = &'b ()> {
    type Current = &'a mut T;
    type Replaced = &'b mut T;
}
\end{lstlisting}
although I have not come up with a good way to utilize this.

\chapter{Conclusion}

The stated goal has been to ``break new gound'' and showcase the potential innovations
a formalized array-operation semantics can build. To summarize:

\begin{itemize}
  \item We have seen a minimal set of operations on arrays by studing natural
    numbered mutiplication:
    \begin{itemize}
      \item Shape desription (the notion of an array domain being given as a list of `axes', corresponding
        to associativity)
      \item Transpose (re-ordering the axes of the array domain, corresponding to commutativity)
      \item Diagonal (eliminating equal axes, corresponding to square-free numbers)
      \item Reduce (eliminating axes, corresponding to division by divisor)
      \item Tiling (creating impotent axes to pad the shape of an array, corresponding to multiplication
        by known constant)
      \item Outer product (combinatoric combination of the elements of two arrays, corresponding to
        multiplication)
    \end{itemize}
  \item We have defined a variety of useful operations:
    \begin{itemize}
      \item Map (applying a function to every entry in an array)
      \item Reduce/fold (quantified sum over an axis)
      \item Outer product (binary function application)
    \end{itemize}
  \item We have identified universal properties:
    \begin{itemize}
      \item The outer product operation with a binary combining function can be decomposed
        into the outer product with the simple pairing function, followed by map with the combining
        function.
      \item The reduce operation with a monoid can be decomposed into recution over the
        free monoid, followed by map with a monoid homomorphism.
    \end{itemize}
\end{itemize}

There are a number of open questions still:
\begin{itemize}
  \item Reduce with the free monoid creates arrays-of-lists, and lists are rank-1 arrays. What
    is the relation to the operation of cutting an array into sub-arrays, like viewing a matrix
    as a vector of row or column-vectors?
  \item What is the exact nature of operations that do not preserve the value-at-origin,
    versus those that do?
\end{itemize}

And the greatest theoretical hurdle left is to actually formalize the category of arrays
and identify its nature (e.g. is it cartesian monodial? is it a topos?)

The other goal was to showcase the power of one identified mathematical fact, namely that
operations on arrays that do not change the codomain, are representable by precomposing with
linear or affine transformations. This has been shown in theory, but the practical implementation
falls short of achieving experimental support for the utility of this idea --- more work is needed
here, as the code part of this project amounts to little more than
an in-depth exploration of novel issues in using Rust.

For a bachelor's thesis, the scope of this subject has grown perhaps a little beyond the guideline
of approximately twenty pages. It does provide a nice starting point for futher projects outside
course scope (sic. projekt uden for kursusregi,) academic publications, and possibly a Masters Thesis.

I have also corresponded with one Justin R. Slepak at Noreastern University,
Boston MA\footnote{\url{https://www.ccis.northeastern.edu/people/justin-slepak/}} who is
working on rank-polymorphism in array languages, about the viability of the idea of a `minimal set of
operatioons' and he expressed some enthusiasm for the idea.

Formalization of multidimensional array semnatics, and multidimentional array slices presented via
affine transformations are both powerful ideas and merits much futher study. A useful multidimensional
array library with static typing, good memory performance, and sound semantics now seem closer than
ever.

\section{Acknowledgements}

I would like to thank my thesis advisor Jyrki Katajainen for his steady and dependable
support in writing this thesis.

I would like to thank my friend and alumni Mathias Svensson for code review assistance and
proofreading of formulas.

I would like to thank my significant other for putting up with me for these six months.

\clearpage
\phantomsection
\addcontentsline{toc}{chapter}{References}
\printbibliography

\vfill
\begin{center}\itshape End report.\end{center}
\clearpage

\appendix
\renewcommand\thesection{\Alph{section}}
\phantomsection
\addcontentsline{toc}{chapter}{Appendices}


\section{Notation guide}

Array specific:
\begin{description}[align=left,labelwidth=7em]
  \item[\(\sharp A\)] Shape of an array (Definition~\ref{def:shape})
  \item[\(\flat A\)] Flat/in-order traversal of an array (Definition~\ref{def:flat})
  \item[\(\varrho(A, \rho)\)] Reshape of array (Definition~\ref{def:reshape})
  \item[\(A \circ \sigma\)] Generalized transpose of array (Definition~\ref{def:transpose})
  \item[\(f \circ A\)] Element-wise mapping of array (Definition~\ref{def:map})
  \item[\(\tile_{\times k, x} A\)] Tiling an array (Definition~\ref{def:tiling})
  \item[\(\diag_{x, y} A\)] Diagnoally traverse an array (Definition~\ref{def:diagonal})
  \item[\(\reduce^k_{(S, \oplus, \epsilon)} A\)] Reducing an array by a monoid (Definition~\ref{def:reduce})
  \item[\(A \oprodby f B\)] Outer product of arrays (Definition~\ref{def:outer-product})
\end{description}

Generic:
\begin{description}
  \item[\(\sum f\)] implicitly \(sum_{i=0}^{k} f(i)\) where applicable (Remark~\ref{rem:quant-elision})
\end{description}

\section{Code}
\label{sec:apx-code}
\lstset{ 
  backgroundcolor=\color{white},  
  basicstyle=\ttfamily,      
  breakatwhitespace=false,       
  breaklines=true,            
  captionpos=b,              
  commentstyle=\color{black!50}, 
  keepspaces=true,
  keywordstyle=\bfseries,
  language=Rust,
  numbers=left,                   
  numbersep=5pt,                  
  numberstyle=\tiny\ttfamily, 
  rulecolor=\color{black}, 
  showspaces=false,       
  showstringspaces=false,
  showtabs=false,       
  stepnumber=1,
  stringstyle=\color{black!70}, 
  tabsize=2,
  title=\lstname,
  belowskip=0em,
}

Total lines of code in the project come out in excess of 1500. Be warned, this appendix
is very long.

Every Rust project is usually centered around a single file that names the
other code files:
\lstinputlisting{../cat_arrays/src/lib.rs}

Core trait definitions:
\lstinputlisting{../cat_arrays/src/traits.rs}

The macro system exposed directly in rust
source code is hygeinic and quite limited in scope. Macros accomplished by add-on compiler modules
(called procedural macros) are of course turing complete.
I use macros to eliminate some boilerplate:
\lstinputlisting{../cat_arrays/src/macros.rs}

A utility type to enable rank-1 arrays to integrate with Rust's iterator libraries:
\lstinputlisting{../cat_arrays/src/iter.rs}

Utility functions to aid in computing e.g. array indexes:
\lstinputlisting{../cat_arrays/src/utils.rs}

A dummy implementation of the array traits:
\lstinputlisting{../cat_arrays/src/dummy.rs}

The most basic array type, the singleton. Notice here the technique of
re-casting using \texttt{std::mem::transmute} in situations where the lifetimes
don't align but we know it is safe:
\lstinputlisting{../cat_arrays/src/unit.rs}

The constant function as an array type: Notice the similarity in
structure with \texttt{Unit} and the use of a \texttt{Vec<usize>} as shape.
\lstinputlisting{../cat_arrays/src/constant.rs}

An implementation for built-in types supporting slices, allowing \texttt{Vec<usize>}
to be used as an array for shape descriptions.
\lstinputlisting{../cat_arrays/src/constant.rs}

Generic reshape wrappers:
\lstinputlisting{../cat_arrays/src/gen_reshape.rs}

Generic slice wrappers (unfinished):
\lstinputlisting{../cat_arrays/src/gen_slice.rs}

Lazy map implementation:
\lstinputlisting{../cat_arrays/src/map.rs}

Lazy fold implementation:
\lstinputlisting{../cat_arrays/src/fold.rs}

Lazy outer product implementation (unfinished):
\lstinputlisting{../cat_arrays/src/outer_prod.rs}

\vfill
\begin{center}\itshape End appendices.\end{center}
\clearpage

%\listoftables
%\listoffigures
%\printindex

\vfill
\begin{center}\itshape End document.\end{center}

\end{document}
